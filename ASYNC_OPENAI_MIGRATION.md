# async-openai åº“è¿ç§»æ€»ç»“

## ğŸ“‹ è¿ç§»æ¦‚è¿°

ä» `openai` crate è¿ç§»åˆ° `async-openai` crateï¼Œä»¥è·å¾—æ›´å¥½çš„å¼‚æ­¥æ”¯æŒå’Œæ›´ä¸°å¯Œçš„åŠŸèƒ½ã€‚

**è¿ç§»æ—¥æœŸ**: 2024
**åŸå› **: `async-openai` æä¾›äº†æ›´ç°ä»£çš„ APIã€æ›´å¥½çš„ç±»å‹æ”¯æŒå’Œæ›´å®Œå–„çš„æ–‡æ¡£

## ğŸ”„ è¿ç§»å‰åå¯¹æ¯”

### æ—§åº“ (openai 1.1.1)

```toml
[dependencies]
openai = "1.1.1"
```

```rust
use openai::chat::{ChatCompletion, ChatCompletionMessage, ChatCompletionMessageRole};
use openai::Credentials;

let credentials = Credentials::new(&api_key, &api_base_url);
let chat_completion = ChatCompletion::builder(&model_name, messages)
    .credentials(credentials)
    .create()
    .await?;
```

**é—®é¢˜**:
- âŒ æ–‡æ¡£ä¸å¤Ÿå®Œå–„
- âŒ ç±»å‹å®šä¹‰ä¸å¤Ÿæ¸…æ™°
- âŒ ç¼ºå°‘ä¸€äº›ç°ä»£åŒ–åŠŸèƒ½
- âŒ æ›´æ–°è¾ƒæ…¢

### æ–°åº“ (async-openai 0.32.2)

```toml
[dependencies]
async-openai = { version = "0.32.2", features = ["_api", "chat-completion"] }
```

```rust
use async_openai::{
    config::OpenAIConfig,
    types::chat::{
        ChatCompletionRequestMessage, 
        ChatCompletionRequestSystemMessageArgs,
        ChatCompletionRequestUserMessageArgs, 
        CreateChatCompletionRequestArgs,
    },
    Client,
};

// é…ç½®å®¢æˆ·ç«¯
let config = OpenAIConfig::new()
    .with_api_key(&api_key)
    .with_api_base(&api_base_url);
let client = Client::with_config(config);

// æ„å»ºæ¶ˆæ¯
let system_msg = ChatCompletionRequestSystemMessageArgs::default()
    .content(system_message)
    .build()?;
let user_msg = ChatCompletionRequestUserMessageArgs::default()
    .content(user_message)
    .build()?;

let mut messages = vec![
    ChatCompletionRequestMessage::System(system_msg),
    ChatCompletionRequestMessage::User(user_msg),
];

// åˆ›å»ºè¯·æ±‚
let request = CreateChatCompletionRequestArgs::default()
    .model(&model_name)
    .messages(messages)
    .temperature(0.3)
    .max_tokens(1024u32)
    .build()?;

// è°ƒç”¨ API
let response = client.chat().create(request).await?;
```

**ä¼˜åŠ¿**:
- âœ… æ›´æ¸…æ™°çš„ API è®¾è®¡ï¼ˆä½¿ç”¨ Builder æ¨¡å¼ï¼‰
- âœ… æ›´å®Œå–„çš„ç±»å‹ç³»ç»Ÿ
- âœ… æ”¯æŒæ›´å¤š OpenAI åŠŸèƒ½
- âœ… æ›´å¥½çš„é”™è¯¯å¤„ç†
- âœ… æ´»è·ƒçš„ç¤¾åŒºç»´æŠ¤
- âœ… æ”¯æŒ OpenAI å…¼å®¹æœåŠ¡ï¼ˆAzure, Gemini, Doubao ç­‰ï¼‰

## ğŸ”§ è¿ç§»æ­¥éª¤

### 1. æ›´æ–° Cargo.toml

```diff
[dependencies]
- openai = "1.1.1"
+ async-openai = { version = "0.32.2", features = ["_api", "chat-completion"] }
```

**Feature è¯´æ˜**:
- `_api`: å¯ç”¨æ ¸å¿ƒ API åŠŸèƒ½ï¼ˆå¿…éœ€ï¼‰
- `chat-completion`: å¯ç”¨èŠå¤©å®ŒæˆåŠŸèƒ½ï¼ˆå¿…éœ€ï¼‰

### 2. æ›´æ–°å¯¼å…¥è¯­å¥

```diff
- use openai::chat::{ChatCompletion, ChatCompletionMessage, ChatCompletionMessageRole};
- use openai::Credentials;
+ use async_openai::{
+     config::OpenAIConfig,
+     types::chat::{
+         ChatCompletionRequestMessage,
+         ChatCompletionRequestSystemMessageArgs,
+         ChatCompletionRequestUserMessageArgs,
+         CreateChatCompletionRequestArgs,
+     },
+     Client,
+ };
```

### 3. é‡æ„æœåŠ¡ç»“æ„

#### æ—§ä»£ç ç»“æ„

```rust
pub struct LlmService {
    api_key: String,
    api_base_url: String,
    model_name: String,
}

impl LlmService {
    pub fn new(config: &Config) -> Self {
        Self {
            api_key: config.llm_api_key.clone(),
            api_base_url: config.llm_api_base_url.clone(),
            model_name: config.llm_model_name.clone(),
        }
    }
}
```

#### æ–°ä»£ç ç»“æ„

```rust
pub struct LlmService {
    client: Client<OpenAIConfig>,  // âœ… ä½¿ç”¨é…ç½®å¥½çš„å®¢æˆ·ç«¯
    model_name: String,
}

impl LlmService {
    pub fn new(config: &Config) -> Self {
        // é…ç½® OpenAI å®¢æˆ·ç«¯
        let openai_config = OpenAIConfig::new()
            .with_api_key(&config.llm_api_key)
            .with_api_base(&config.llm_api_base_url);
        
        let client = Client::with_config(openai_config);
        
        Self {
            client,
            model_name: config.llm_model_name.clone(),
        }
    }
}
```

### 4. æ›´æ–° API è°ƒç”¨ä»£ç 

#### æ—§ä»£ç 

```rust
async fn call_llm_api(&self, prompt: &str) -> Result<String> {
    let credentials = Credentials::new(&self.api_key, &self.api_base_url);
    
    let mut messages = Vec::new();
    messages.push(ChatCompletionMessage {
        role: ChatCompletionMessageRole::User,
        content: Some(prompt.to_string()),
        name: None,
        function_call: None,
        tool_call_id: None,
        tool_calls: None,
    });
    
    let chat_completion = ChatCompletion::builder(&self.model_name, messages)
        .credentials(credentials)
        .create()
        .await?;
    
    let content = chat_completion
        .choices
        .first()
        .and_then(|c| c.message.content.clone())
        .ok_or_else(|| anyhow::anyhow!("Empty response"))?;
    
    Ok(content)
}
```

#### æ–°ä»£ç 

```rust
pub async fn send_to_llm(
    &self,
    user_message: &str,
    system_message: Option<&str>,
) -> Result<String> {
    // æ„å»ºæ¶ˆæ¯åˆ—è¡¨
    let mut messages = Vec::new();
    
    // æ·»åŠ ç³»ç»Ÿæ¶ˆæ¯ï¼ˆå¦‚æœæä¾›ï¼‰
    if let Some(sys_msg) = system_message {
        let system_msg = ChatCompletionRequestSystemMessageArgs::default()
            .content(sys_msg)
            .build()?;
        messages.push(ChatCompletionRequestMessage::System(system_msg));
    }
    
    // æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
    let user_msg = ChatCompletionRequestUserMessageArgs::default()
        .content(user_message)
        .build()?;
    messages.push(ChatCompletionRequestMessage::User(user_msg));
    
    // æ„å»ºè¯·æ±‚
    let request = CreateChatCompletionRequestArgs::default()
        .model(&self.model_name)
        .messages(messages)
        .temperature(0.3)
        .max_tokens(1024u32)
        .build()?;
    
    // è°ƒç”¨ API
    let response = self.client.chat().create(request).await
        .map_err(|e| anyhow::anyhow!("LLM API è°ƒç”¨å¤±è´¥: {}", e))?;
    
    // æå–å“åº”å†…å®¹
    let content = response
        .choices
        .first()
        .and_then(|choice| choice.message.content.clone())
        .ok_or_else(|| anyhow::anyhow!("LLM è¿”å›å†…å®¹ä¸ºç©º"))?;
    
    Ok(content.trim().to_string())
}
```

## ğŸ¯ æ ¸å¿ƒæ”¹è¿›

### 1. é€šç”¨çš„ LLM æ¥å£

æ–°æ¶æ„æä¾›äº†ä¸€ä¸ªé€šç”¨çš„ `send_to_llm` å‡½æ•°ä½œä¸ºåŸºç¡€ï¼š

```rust
// é€šç”¨æ¥å£
pub async fn send_to_llm(
    &self,
    user_message: &str,
    system_message: Option<&str>,
) -> Result<String>

// ä¸“ç”¨æ¥å£ï¼ˆåŸºäºé€šç”¨æ¥å£ï¼‰
pub async fn find_best_match(
    &self,
    search_results: &[SearchResult],
    stem: &str,
    imgs: Option<&[String]>,
) -> Result<usize>
```

**ä¼˜åŠ¿**:
- âœ… å•ä¸€èŒè´£åŸåˆ™
- âœ… æ˜“äºæµ‹è¯•
- âœ… å¯å¤ç”¨æ€§é«˜
- âœ… æ˜“äºæ‰©å±•æ–°åŠŸèƒ½

### 2. ç±»å‹å®‰å…¨

ä½¿ç”¨ Builder æ¨¡å¼å’Œå¼ºç±»å‹ï¼š

```rust
// âœ… ç¼–è¯‘æ—¶æ£€æŸ¥
let request = CreateChatCompletionRequestArgs::default()
    .model(&self.model_name)        // ç±»å‹: &str
    .messages(messages)              // ç±»å‹: Vec<ChatCompletionRequestMessage>
    .temperature(0.3)                // ç±»å‹: f32
    .max_tokens(1024u32)            // ç±»å‹: u32
    .build()?;                       // è¿”å› Result

// âŒ æ—§æ–¹å¼ï¼šæ‰‹åŠ¨æ„å»º JSONï¼Œå®¹æ˜“å‡ºé”™
let request_body = json!({
    "model": self.model_name,
    "messages": messages,
    "temperature": 0.3,
    "max_tokens": 1024
});
```

### 3. æ›´å¥½çš„é”™è¯¯å¤„ç†

```rust
// âœ… æ¸…æ™°çš„é”™è¯¯ä¿¡æ¯
let response = self.client.chat().create(request).await
    .map_err(|e| {
        warn!("LLM API è°ƒç”¨å¤±è´¥: {}", e);
        anyhow::anyhow!("LLM API è°ƒç”¨å¤±è´¥: {}", e)
    })?;
```

## ğŸ“Š å›¾ç‰‡å¤„ç†è¯´æ˜

### å½“å‰å®ç°ï¼ˆä¸ä½¿ç”¨ Vision APIï¼‰

ç›®å‰çš„å®ç°**ä¸ç›´æ¥å‘é€å›¾ç‰‡å†…å®¹**ï¼Œè€Œæ˜¯ï¼š

1. **åœ¨ prompt ä¸­åŒ…å«å›¾ç‰‡ URL**
2. **è®© LLM é€šè¿‡ URL ç›¸ä¼¼åº¦åˆ¤æ–­**

```rust
let toml_img_info = if let Some(imgs) = imgs {
    if imgs.is_empty() {
        "æ— å›¾ç‰‡".to_string()
    } else {
        let img_list: Vec<String> = imgs
            .iter()
            .enumerate()
            .map(|(i, url)| format!("    å›¾ç‰‡ {}: {}", i + 1, url))
            .collect();
        format!("åŒ…å« {} å¼ å›¾ç‰‡ï¼š\n{}", imgs.len(), img_list.join("\n"))
    }
} else {
    "æ— å›¾ç‰‡".to_string()
};
```

**ä¸ºä»€ä¹ˆè¿™æ ·åšï¼Ÿ**

1. âœ… **æ¨¡å‹å…¼å®¹æ€§**: `doubao-seed-1.6` ç­‰æ¨¡å‹å¯èƒ½ä¸æ”¯æŒ Vision API
2. âœ… **æˆæœ¬è¾ƒä½**: ä¸éœ€è¦å‘é€å›¾ç‰‡å†…å®¹
3. âœ… **è¶³å¤Ÿå‡†ç¡®**: é€šè¿‡ URL çš„æ–‡ä»¶åå’Œè·¯å¾„å¯ä»¥åˆ¤æ–­å›¾ç‰‡æ˜¯å¦ç›¸åŒ
4. âœ… **å®ç°ç®€å•**: ä¸éœ€è¦å¤„ç†å›¾ç‰‡ç¼–ç å’Œä¸Šä¼ 

### æœªæ¥ï¼šæ”¯æŒ Vision APIï¼ˆå¯é€‰ï¼‰

å¦‚æœéœ€è¦çœŸæ­£çš„å›¾ç‰‡ç†è§£åŠŸèƒ½ï¼Œå¯ä»¥ä½¿ç”¨ Vision APIï¼š

```rust
use async_openai::types::chat::{
    ChatCompletionRequestUserMessageContent,
    ImageUrl,
};

// æ„å»ºåŒ…å«å›¾ç‰‡çš„æ¶ˆæ¯
let image_url = ImageUrl {
    url: "https://example.com/image.jpg".to_string(),
    detail: Some("high".to_string()),
};

let content = ChatCompletionRequestUserMessageContent::Array(vec![
    ChatCompletionRequestMessageContentPart::Text(
        ChatCompletionRequestMessageContentPartText {
            text: "è¿™å¼ å›¾ç‰‡ä¸­æœ‰ä»€ä¹ˆï¼Ÿ".to_string(),
        }
    ),
    ChatCompletionRequestMessageContentPart::ImageUrl(
        ChatCompletionRequestMessageContentPartImageUrl {
            image_url,
        }
    ),
]);

let user_msg = ChatCompletionRequestUserMessageArgs::default()
    .content(content)
    .build()?;
```

**æ³¨æ„äº‹é¡¹**:
- éœ€è¦ä½¿ç”¨æ”¯æŒ Vision çš„æ¨¡å‹ï¼ˆå¦‚ `gpt-4-vision-preview`ï¼‰
- æˆæœ¬è¾ƒé«˜
- éœ€è¦å¤„ç†å›¾ç‰‡ç¼–ç ï¼ˆbase64 æˆ– URLï¼‰

## ğŸ§ª æµ‹è¯•æ›´æ–°

### æµ‹è¯•é…ç½®

```rust
fn create_test_service() -> LlmService {
    let config = OpenAIConfig::new()
        .with_api_key("26e96c4d312e48feacbd78b7c42bd71e")
        .with_api_base("http://menshen.xdf.cn/v1");
    
    let client = Client::with_config(config);
    
    LlmService {
        client,
        model_name: "doubao-seed-1.6".to_string(),
    }
}
```

### æµ‹è¯•é€šç”¨ LLM è°ƒç”¨

```bash
cargo test test_send_to_llm_simple -- --ignored --nocapture
```

### æµ‹è¯•é¢˜ç›®åŒ¹é…

```bash
cargo test test_llm_api_connectivity -- --ignored --nocapture
```

## âœ… è¿ç§»æ£€æŸ¥æ¸…å•

- [x] æ›´æ–° `Cargo.toml` ä¾èµ–
- [x] æ›´æ–°å¯¼å…¥è¯­å¥
- [x] é‡æ„ `LlmService` ç»“æ„
- [x] å®ç°é€šç”¨çš„ `send_to_llm` æ–¹æ³•
- [x] æ›´æ–° `find_best_match` æ–¹æ³•
- [x] æ›´æ–°æµ‹è¯•ç”¨ä¾‹
- [x] éªŒè¯ API è°ƒç”¨æ­£å¸¸
- [x] éªŒè¯å›¾ç‰‡å¤„ç†é€»è¾‘
- [x] æ›´æ–°æ–‡æ¡£

## ğŸ“ æœ€ä½³å®è·µ

### 1. ä½¿ç”¨ Builder æ¨¡å¼

```rust
// âœ… æ¨è
let request = CreateChatCompletionRequestArgs::default()
    .model(&self.model_name)
    .messages(messages)
    .temperature(0.3)
    .build()?;

// âŒ é¿å…æ‰‹åŠ¨æ„å»º
```

### 2. å¤ç”¨å®¢æˆ·ç«¯

```rust
// âœ… åœ¨ç»“æ„ä½“ä¸­ä¿å­˜å®¢æˆ·ç«¯
pub struct LlmService {
    client: Client<OpenAIConfig>,
    model_name: String,
}

// âŒ é¿å…æ¯æ¬¡è°ƒç”¨éƒ½åˆ›å»ºæ–°å®¢æˆ·ç«¯
```

### 3. é€‚å½“çš„é”™è¯¯å¤„ç†

```rust
// âœ… æ¸…æ™°çš„é”™è¯¯ä¿¡æ¯
.map_err(|e| {
    warn!("LLM API è°ƒç”¨å¤±è´¥: {}", e);
    anyhow::anyhow!("LLM API è°ƒç”¨å¤±è´¥: {}", e)
})?

// âœ… åˆç†çš„é»˜è®¤å€¼
.ok_or_else(|| anyhow::anyhow!("LLM è¿”å›å†…å®¹ä¸ºç©º"))?
```

### 4. æ—¥å¿—è®°å½•

```rust
debug!("è°ƒç”¨ LLM APIï¼Œæ¨¡å‹: {}", self.model_name);
debug!("ç”¨æˆ·æ¶ˆæ¯é•¿åº¦: {} å­—ç¬¦", user_message.len());
warn!("LLM API è°ƒç”¨å¤±è´¥: {}", e);
```

## ğŸ“š å‚è€ƒèµ„æº

- [async-openai æ–‡æ¡£](https://docs.rs/async-openai/)
- [async-openai GitHub](https://github.com/64bit/async-openai)
- [OpenAI API æ–‡æ¡£](https://platform.openai.com/docs/api-reference)
- [Vision API ç¤ºä¾‹](https://github.com/64bit/async-openai/tree/main/examples/vision-chat)

## ğŸš€ åç»­ä¼˜åŒ–å»ºè®®

1. **æ”¯æŒæµå¼å“åº”** (Streaming)
   - å¯¹äºé•¿æ–‡æœ¬ç”Ÿæˆï¼Œå¯ä»¥ä½¿ç”¨æµå¼ API æé«˜ç”¨æˆ·ä½“éªŒ

2. **æ·»åŠ é‡è¯•æœºåˆ¶**
   - ä½¿ç”¨ `backoff` crate å®ç°æŒ‡æ•°é€€é¿é‡è¯•

3. **ç¼“å­˜æœºåˆ¶**
   - å¯¹äºç›¸åŒçš„ promptï¼Œå¯ä»¥ç¼“å­˜å“åº”ç»“æœ

4. **æ”¯æŒå¤šæ¨¡å‹åˆ‡æ¢**
   - æ ¹æ®ä»»åŠ¡ç±»å‹åŠ¨æ€é€‰æ‹©æ¨¡å‹

5. **æ€§èƒ½ç›‘æ§**
   - è®°å½• API è°ƒç”¨å»¶è¿Ÿå’ŒæˆåŠŸç‡

6. **Token è®¡æ•°**
   - è·Ÿè¸ª token ä½¿ç”¨æƒ…å†µï¼Œä¼˜åŒ–æˆæœ¬

---

**è¿ç§»å®Œæˆæ—¥æœŸ**: 2024
**è¿ç§»æ‰§è¡Œè€…**: Claude AI Assistant
**éªŒè¯çŠ¶æ€**: âœ… ç¼–è¯‘é€šè¿‡ï¼Œâœ… æµ‹è¯•é€šè¿‡ï¼Œâœ… API è°ƒç”¨æ­£å¸¸